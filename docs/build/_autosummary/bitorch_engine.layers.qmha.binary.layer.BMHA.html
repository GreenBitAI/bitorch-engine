<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>bitorch_engine.layers.qmha.binary.layer.BMHA &mdash; Bitorch Engine 0.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-toolbox-code.css?v=4ee5d529" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=938c9ccc"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="bitorch_engine.layers.qmha.binary.layer.LearnableBias" href="bitorch_engine.layers.qmha.binary.layer.LearnableBias.html" />
    <link rel="prev" title="bitorch_engine.layers.qmha.binary.layer" href="bitorch_engine.layers.qmha.binary.layer.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Bitorch Engine
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_options.html">Build options</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../documentation.html">Full Documentation</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="bitorch_engine.html">bitorch_engine</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="bitorch_engine.functions.html">bitorch_engine.functions</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="bitorch_engine.layers.html">bitorch_engine.layers</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="bitorch_engine.layers.qconv.html">bitorch_engine.layers.qconv</a></li>
<li class="toctree-l4"><a class="reference internal" href="bitorch_engine.layers.qembedding.html">bitorch_engine.layers.qembedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="bitorch_engine.layers.qlinear.html">bitorch_engine.layers.qlinear</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="bitorch_engine.layers.qmha.html">bitorch_engine.layers.qmha</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="bitorch_engine.optim.html">bitorch_engine.optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="bitorch_engine.utils.html">bitorch_engine.utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Bitorch Engine</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../documentation.html">Full Documentation</a></li>
          <li class="breadcrumb-item"><a href="bitorch_engine.html">bitorch_engine</a></li>
          <li class="breadcrumb-item"><a href="bitorch_engine.layers.html">bitorch_engine.layers</a></li>
          <li class="breadcrumb-item"><a href="bitorch_engine.layers.qmha.html">bitorch_engine.layers.qmha</a></li>
          <li class="breadcrumb-item"><a href="bitorch_engine.layers.qmha.binary.html">bitorch_engine.layers.qmha.binary</a></li>
          <li class="breadcrumb-item"><a href="bitorch_engine.layers.qmha.binary.layer.html">bitorch_engine.layers.qmha.binary.layer</a></li>
      <li class="breadcrumb-item active">bitorch_engine.layers.qmha.binary.layer.BMHA</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/_autosummary/bitorch_engine.layers.qmha.binary.layer.BMHA.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bitorch-engine-layers-qmha-binary-layer-bmha">
<h1>bitorch_engine.layers.qmha.binary.layer.BMHA<a class="headerlink" href="#bitorch-engine-layers-qmha-binary-layer-bmha" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bitorch_engine.layers.qmha.binary.layer.</span></span><span class="sig-name descname"><span class="pre">BMHA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bitorch_engine/layers/qmha/binary/layer.html#BMHA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA" title="Link to this definition"></a></dt>
<dd><p>Implements a binary version of multi-head attention (MHA) where the linear transformations
are executed using binary operations to improve efficiency. This class is designed to work
with binary weights and can be particularly useful for deployments in resource-constrained
environments or for models where computational efficiency is crucial.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.dtype" title="Link to this definition"></a></dt>
<dd><p>Data type for the computations, typically float or binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.dtype</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.num_heads">
<span class="sig-name descname"><span class="pre">num_heads</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.num_heads" title="Link to this definition"></a></dt>
<dd><p>Number of attention heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.head_dim">
<span class="sig-name descname"><span class="pre">head_dim</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.head_dim" title="Link to this definition"></a></dt>
<dd><p>Dimension of each attention head.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.hidden_dim">
<span class="sig-name descname"><span class="pre">hidden_dim</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.hidden_dim" title="Link to this definition"></a></dt>
<dd><p>Dimension of the hidden layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.input_dim">
<span class="sig-name descname"><span class="pre">input_dim</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.input_dim" title="Link to this definition"></a></dt>
<dd><p>Dimension of the input layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.q_linear">
<span class="sig-name descname"><span class="pre">q_linear</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.q_linear" title="Link to this definition"></a></dt>
<dd><p>Linear transformation for the query vector using binary operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass.html#bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass" title="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass">BinaryLinearCutlass</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.v_linear">
<span class="sig-name descname"><span class="pre">v_linear</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.v_linear" title="Link to this definition"></a></dt>
<dd><p>Linear transformation for the value vector using binary operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass.html#bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass" title="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass">BinaryLinearCutlass</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.k_linear">
<span class="sig-name descname"><span class="pre">k_linear</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.k_linear" title="Link to this definition"></a></dt>
<dd><p>Linear transformation for the key vector using binary operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass.html#bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass" title="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass">BinaryLinearCutlass</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.dropout" title="Link to this definition"></a></dt>
<dd><p>Dropout layer to prevent overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.out">
<span class="sig-name descname"><span class="pre">out</span></span><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.out" title="Link to this definition"></a></dt>
<dd><p>Final linear layer to project the attention output back to input dimensionality.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass.html#bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass" title="bitorch_engine.layers.qlinear.binary.cutlass.layer.BinaryLinearCutlass">BinaryLinearCutlass</a></p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the hidden size is not a multiple of the number of attention heads.</p>
</dd>
</dl>
<p>Note: The implementation of this class is still in the EXPERIMENTAL STAGE!</p>
<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.__init__" title="bitorch_engine.layers.qmha.binary.layer.BMHA.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a></p></td>
<td><p>Initializes the BMHA module with the specified parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.forward" title="bitorch_engine.layers.qmha.binary.layer.BMHA.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a></p></td>
<td><p>Forward pass for the binary multi-head attention layer.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">training</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bitorch_engine/layers/qmha/binary/layer.html#BMHA.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes the BMHA module with the specified parameters. It sets up the binary linear layers for
the queries, keys, and values, along with the output projection layer. It also validates that the
hidden dimension is evenly divisible by the number of heads to ensure that the dimensions align
properly for multi-head attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – The size of each input vector.</p></li>
<li><p><strong>hidden_dim</strong> (<em>int</em>) – The size of the hidden dimension. Must be divisible by num_heads.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) – The number of attention heads.</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em>, </em><em>optional</em>) – The data type for computations. Defaults to torch.float.</p></li>
<li><p><strong>*args</strong> – Variable length argument list for the parent class.</p></li>
<li><p><strong>**kwargs</strong> – Arbitrary keyword arguments for the parent class.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If hidden_dim is not divisible by num_heads, an error is raised to alert the user.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bitorch_engine.layers.qmha.binary.layer.BMHA.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/bitorch_engine/layers/qmha/binary/layer.html#BMHA.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bitorch_engine.layers.qmha.binary.layer.BMHA.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass for the binary multi-head attention layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_states</strong> (<em>torch.Tensor</em>) – Input tensor of shape (batch_size, sequence_length, input_dim).</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Optional mask tensor to exclude certain positions from
the attention mechanism. Shape (batch_size, sequence_length).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple containing:</dt><dd><ul class="simple">
<li><p>Output tensor of shape (batch_size, sequence_length, input_dim).</p></li>
<li><p>Attention scores tensor of shape (batch_size, num_heads, sequence_length, sequence_length).</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="bitorch_engine.layers.qmha.binary.layer.html" class="btn btn-neutral float-left" title="bitorch_engine.layers.qmha.binary.layer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bitorch_engine.layers.qmha.binary.layer.LearnableBias.html" class="btn btn-neutral float-right" title="bitorch_engine.layers.qmha.binary.layer.LearnableBias" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Haojin Yang, Joseph Bethge, Nianhui Guo, Maximilian Schulze, Hong Guo, Paul Mattes.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
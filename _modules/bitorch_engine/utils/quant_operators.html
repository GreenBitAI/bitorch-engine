<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>bitorch_engine.utils.quant_operators &mdash; Bitorch Engine 0.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-toolbox-code.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=938c9ccc"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            Bitorch Engine
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../build_options.html">Build options</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../documentation.html">Full Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Bitorch Engine</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">bitorch_engine.utils.quant_operators</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for bitorch_engine.utils.quant_operators</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>


<div class="viewcode-block" id="nv_tensor_quant">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.nv_tensor_quant.html#bitorch_engine.utils.quant_operators.nv_tensor_quant">[docs]</a>
<span class="k">def</span> <span class="nf">nv_tensor_quant</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">amax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">unsigned</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizes the given tensor using specified quantization parameters. This method supports</span>
<span class="sd">    both signed and unsigned quantization with an option for narrow range quantization.</span>
<span class="sd">    This function is shared between TensorQuantFunction and FakeTensorQuantFunction.</span>

<span class="sd">    Author: nv_pytorch_quantization</span>
<span class="sd">    Source: https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/pytorch_quantization/tensor_quant.py#L315</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (torch.Tensor): The input tensor to be quantized.</span>
<span class="sd">        amax (torch.Tensor or None): The maximum absolute value used for quantization scaling. If None, it will be</span>
<span class="sd">                                      calculated from the input tensor.</span>
<span class="sd">        num_bits (int): Number of bits to use for quantization, default is 8.</span>
<span class="sd">        unsigned (bool): Flag indicating if the quantization is unsigned, default is False.</span>
<span class="sd">        narrow_range (bool): Flag indicating if the quantization should use narrow range, default is True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `amax` has a different shape than `inputs` or contains negative values.</span>
<span class="sd">        TypeError: If negative values are encountered in unsigned quantization mode.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The quantized tensor.</span>
<span class="sd">        torch.Tensor: The scale factor used for quantization.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Quantization is performed in FP32 to avoid overflow.</span>
<span class="sd">        - If `inputs` or `amax` are in FP16 or BF16, they are converted to FP32 for calculation.</span>
<span class="sd">        - The quantization range is adjusted based on `unsigned` and `narrow_range` flags.</span>
<span class="sd">        - Special handling for `amax` values smaller than the minimum representable value of FP16.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">amax</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="n">amax</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;amax </span><span class="si">%s</span><span class="s2"> has different shape than inputs </span><span class="si">%s</span><span class="s2">. Make sure broadcast works as expected!&quot;</span><span class="p">,</span>
            <span class="n">amax</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
        <span class="p">)</span>

    <span class="c1"># print(&quot;{} bits quantization on shape {} tensor.&quot;.format(num_bits, inputs.size()))</span>

    <span class="k">if</span> <span class="n">amax</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">unsigned</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Negative values encountered in unsigned quantization.&quot;</span><span class="p">)</span>

    <span class="c1"># Computation must be in FP32 to prevent potential over flow.</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">or</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">amax</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">or</span> <span class="n">amax</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">amax</span> <span class="o">=</span> <span class="n">amax</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">min_amax</span> <span class="o">=</span> <span class="n">amax</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">min_amax</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Negative values in amax&quot;</span><span class="p">)</span>

    <span class="n">max_bound</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="p">(</span><span class="mf">2.0</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">unsigned</span><span class="p">)))</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">unsigned</span><span class="p">:</span>
        <span class="n">min_bound</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">narrow_range</span><span class="p">:</span>
        <span class="n">min_bound</span> <span class="o">=</span> <span class="o">-</span><span class="n">max_bound</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">min_bound</span> <span class="o">=</span> <span class="o">-</span><span class="n">max_bound</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">max_bound</span> <span class="o">/</span> <span class="n">amax</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">((</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">round_</span><span class="p">(),</span> <span class="n">min_bound</span><span class="p">,</span> <span class="n">max_bound</span><span class="p">)</span>

    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">24</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">min_amax</span> <span class="o">&lt;=</span> <span class="n">epsilon</span><span class="p">:</span>  <span class="c1"># Treat amax smaller than minimum representable of fp16 0</span>
        <span class="n">zero_amax_mask</span> <span class="o">=</span> <span class="n">amax</span> <span class="o">&lt;=</span> <span class="n">epsilon</span>
        <span class="n">scale</span><span class="p">[</span><span class="n">zero_amax_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Value quantized with amax=0 should all be 0</span>
    <span class="k">if</span> <span class="n">min_amax</span> <span class="o">&lt;=</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">scale</span><span class="p">[</span>
            <span class="n">zero_amax_mask</span>
        <span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Return 1 makes more sense for values quantized to 0 with amax=0</span>

    <span class="k">if</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="ow">or</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">scale</span></div>



<div class="viewcode-block" id="bit_set">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.bit_set.html#bitorch_engine.utils.quant_operators.bit_set">[docs]</a>
<span class="k">def</span> <span class="nf">bit_set</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets a specific bit in an integer variable to a given value.</span>

<span class="sd">    This method allows you to modify a single bit within an integer by shifting the `val` (either 0 or 1) to the position `pos` and then performing a bitwise OR operation with the original variable `var`. This effectively sets the bit at position `pos` to the value specified by `val`.</span>

<span class="sd">    The operation performed is equivalent to:</span>
<span class="sd">    `var |= (val &lt;&lt; pos)`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        var (int): The original integer variable whose bit is to be modified.</span>
<span class="sd">        pos (int): The position of the bit to be set, starting from 0 for the least significant bit (LSB).</span>
<span class="sd">        val (int): The new value for the bit, either 0 or 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: The modified integer with the bit at position `pos` set to `val`.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; bit_set(0b0010, 1, 1)</span>
<span class="sd">        6  # The binary representation is 0b0110</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">var</span> <span class="o">|=</span> <span class="n">val</span> <span class="o">&lt;&lt;</span> <span class="n">pos</span>
    <span class="k">return</span> <span class="n">var</span></div>



<div class="viewcode-block" id="get_binary_row">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.get_binary_row.html#bitorch_engine.utils.quant_operators.get_binary_row">[docs]</a>
<span class="k">def</span> <span class="nf">get_binary_row</span><span class="p">(</span><span class="n">nd_row</span><span class="p">,</span> <span class="n">binary_row</span><span class="p">,</span> <span class="n">nd_size</span><span class="p">,</span> <span class="n">bits_per_binary_word</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Binarizes an input NDArray (nd_row) into a binary representation (binary_row) based on the specified number of bits per binary word (bits_per_binary_word).</span>

<span class="sd">    This function iteratively processes each segment of the input array with the length of &#39;bits_per_binary_word&#39;, converting each segment into a binary word.</span>
<span class="sd">    Each bit in the binary word represents the sign (positive or negative) of the corresponding element in the input array segment.</span>

<span class="sd">    Specifically, for each segment:</span>
<span class="sd">        - A binary word (&#39;rvalue&#39;) is initialized to 0.</span>
<span class="sd">        - For each element in the segment, if the element is non-negative, the corresponding bit in &#39;rvalue&#39; is set to 1; otherwise, it remains 0.</span>
<span class="sd">        - The binary word is then stored in &#39;binary_row&#39; at the position corresponding to the segment index.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        nd_row (array-like): The input array to be binarized.</span>
<span class="sd">        binary_row (array-like): The output array where each element is a binary word representing a segment of &#39;nd_row&#39;.</span>
<span class="sd">        nd_size (int): The size of the &#39;nd_row&#39; array.</span>
<span class="sd">        bits_per_binary_word (int): The number of bits in each binary word, determining the segment size for binarization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        array-like: The binarized representation of &#39;nd_row&#39; stored in &#39;binary_row&#39;.</span>

<span class="sd">    Example of equivalent C++ logic:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        for (int i = 0; i &lt; size; i+=BITS_PER_BINARY_WORD) {</span>
<span class="sd">          BINARY_WORD rvalue=0;</span>
<span class="sd">          BINARY_WORD sign;</span>
<span class="sd">          for (int j = 0;j &lt; BITS_PER_BINARY_WORD; ++j) {</span>
<span class="sd">            sign = (row[i+j]&gt;=0);</span>
<span class="sd">            BIT_SET(rvalue, j, sign);</span>
<span class="sd">          }</span>
<span class="sd">          b_row[i/BITS_PER_BINARY_WORD] = rvalue;</span>
<span class="sd">        }</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nd_size</span><span class="p">:</span>
        <span class="n">rvalue</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">bits_per_binary_word</span><span class="p">:</span>
            <span class="n">sign</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">nd_row</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">sign</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">rvalue</span> <span class="o">=</span> <span class="n">bit_set</span><span class="p">(</span><span class="n">rvalue</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">sign</span><span class="p">)</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># print(&#39;{0:64b}&#39;.format(rvalue))</span>

        <span class="n">binary_row</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="n">bits_per_binary_word</span><span class="p">)]</span> <span class="o">=</span> <span class="n">rvalue</span>

        <span class="c1"># print(&#39;{0:64b}&#39;.format(binary_row[int(i/bits_per_binary_word)]))</span>
        <span class="c1"># testing stuff</span>
        <span class="c1"># d = mx.nd.array(binary_row, dtype=&quot;float64&quot;)</span>
        <span class="c1"># print(&#39;{0:64b}&#39;.format(int(d.asnumpy()[int(i/bits_per_binary_word)])))</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">bits_per_binary_word</span>
    <span class="k">return</span> <span class="n">binary_row</span></div>



<div class="viewcode-block" id="get_binary_col">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.get_binary_col.html#bitorch_engine.utils.quant_operators.get_binary_col">[docs]</a>
<span class="k">def</span> <span class="nf">get_binary_col</span><span class="p">(</span><span class="n">nd_col</span><span class="p">,</span> <span class="n">binary_col</span><span class="p">,</span> <span class="n">dim_n</span><span class="p">,</span> <span class="n">dim_k</span><span class="p">,</span> <span class="n">bits_per_binary_word</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Binarizes an array column-wise, transforming each element into a binary representation.</span>

<span class="sd">    This function is a Python re-implementation of an equivalent C++ version. It operates on</span>
<span class="sd">    a columnar slice of an array, encoding each segment of BITS_PER_BINARY_WORD bits into</span>
<span class="sd">    a binary word, where each bit is determined by the sign (positive or non-negative vs. negative)</span>
<span class="sd">    of the corresponding element in the input array.</span>

<span class="sd">    The binarization process proceeds by iterating over the array in blocks of BITS_PER_BINARY_WORD,</span>
<span class="sd">    setting each bit based on the sign of the corresponding element. The result is stored in a</span>
<span class="sd">    pre-allocated array for binary representations.</span>

<span class="sd">    Args:</span>
<span class="sd">        nd_col (array-like): The input array containing numerical values to be binarized.</span>
<span class="sd">        binary_col (array-like): Pre-allocated array where the binary representations are stored.</span>
<span class="sd">        dim_n (int): The size of the dimension over which to iterate, typically the number of rows in the array.</span>
<span class="sd">        dim_k (int): The size of the second dimension, typically the number of columns.</span>
<span class="sd">        bits_per_binary_word (int): The number of bits in each binary word, determining the block size for binarization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        array-like: The modified binary_col array containing the binary representations of the input array, column-wise.</span>

<span class="sd">    Example of equivalent C++ logic:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        for(int y=0; y&lt;(n/BITS_PER_BINARY_WORD); y++){</span>
<span class="sd">            for(int x=0; x &lt; k; ++x){</span>
<span class="sd">                BINARY_WORD rvalue=0;</span>
<span class="sd">                BINARY_WORD sign;</span>
<span class="sd">                for(int b=0; b&lt;BITS_PER_BINARY_WORD; ++b){</span>
<span class="sd">                    sign = (col[(y*BITS_PER_BINARY_WORD+b)*k + x]&gt;=0);</span>
<span class="sd">                    BIT_SET(rvalue, b, sign);</span>
<span class="sd">                }</span>
<span class="sd">                b_col[y*k + x] = rvalue;</span>
<span class="sd">            }</span>
<span class="sd">        }</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="nb">int</span><span class="p">(</span><span class="n">dim_n</span> <span class="o">/</span> <span class="n">bits_per_binary_word</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">dim_k</span><span class="p">:</span>
            <span class="n">rvalue</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">bits_per_binary_word</span><span class="p">:</span>
                <span class="n">sign</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">nd_col</span><span class="p">[(</span><span class="n">y</span> <span class="o">*</span> <span class="n">bits_per_binary_word</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">dim_k</span> <span class="o">+</span> <span class="n">x</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">sign</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">rvalue</span> <span class="o">=</span> <span class="n">bit_set</span><span class="p">(</span><span class="n">rvalue</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">sign</span><span class="p">)</span>
                <span class="n">b</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">binary_col</span><span class="p">[</span><span class="n">y</span> <span class="o">*</span> <span class="n">dim_k</span> <span class="o">+</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">rvalue</span>
            <span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">binary_col</span></div>



<div class="viewcode-block" id="q8_quantization">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.q8_quantization.html#bitorch_engine.utils.quant_operators.q8_quantization">[docs]</a>
<span class="k">def</span> <span class="nf">q8_quantization</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scale_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizes an input tensor to 8-bit integers using uniform quantization.</span>

<span class="sd">    The function first ensures that the input tensor is of floating-point type.</span>
<span class="sd">    It then adjusts the scale factor `scale_a` to avoid division by values too close to zero,</span>
<span class="sd">    applying a lower threshold defined by `eps`. The quantization process scales the input tensor</span>
<span class="sd">    by the inverse of `scale_a`, rounds the result to the nearest integer, and clamps the values</span>
<span class="sd">    to the 8-bit range [-128, 127].</span>

<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): The input tensor to be quantized. Should ideally be of floating-point type.</span>
<span class="sd">        scale_a (torch.Tensor): The scale factor for quantization. Each element in `scale_a`</span>
<span class="sd">                                scales the corresponding element in `input`.</span>
<span class="sd">        eps (torch.Tensor): A small positive tensor used to prevent division by zero or values</span>
<span class="sd">                            too close to zero in the scale factor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The quantized tensor, with values rounded and clamped to fit within</span>
<span class="sd">                      the 8-bit integer range.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">is_scale_none</span> <span class="o">=</span> <span class="n">scale_a</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale_a</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="mf">11.269</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.00001</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">scale_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scale_a</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">,</span> <span class="n">scale_a</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">Qn</span> <span class="o">=</span> <span class="o">-</span><span class="mi">128</span>
    <span class="n">Qp</span> <span class="o">=</span> <span class="mi">127</span>
    <span class="k">if</span> <span class="n">is_scale_none</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">scale_a</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">Qn</span><span class="p">,</span> <span class="n">Qp</span><span class="p">),</span> <span class="n">scale_a</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">scale_a</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">Qn</span><span class="p">,</span> <span class="n">Qp</span><span class="p">)</span></div>



<div class="viewcode-block" id="q4_quantization">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.q4_quantization.html#bitorch_engine.utils.quant_operators.q4_quantization">[docs]</a>
<span class="k">def</span> <span class="nf">q4_quantization</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">scale_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantizes an input tensor to 4-bit integers using uniform quantization.</span>

<span class="sd">    The function first ensures that the input tensor is of floating-point type.</span>
<span class="sd">    It then adjusts the scale factor `scale_a` to avoid division by values too close to zero,</span>
<span class="sd">    applying a lower threshold defined by `eps`. The quantization process scales the input tensor</span>
<span class="sd">    by the inverse of `scale_a`, rounds the result to the nearest integer, and clamps the values</span>
<span class="sd">    to the 4-bit range [-8, 7].</span>

<span class="sd">    Args:</span>
<span class="sd">        input (torch.Tensor): The input tensor to be quantized. Should ideally be of floating-point type.</span>
<span class="sd">        scale_a (torch.Tensor): The scale factor for quantization. Each element in `scale_a`</span>
<span class="sd">                                scales the corresponding element in `input`.</span>
<span class="sd">        eps (torch.Tensor): A small positive tensor used to prevent division by zero or values</span>
<span class="sd">                            too close to zero in the scale factor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The quantized tensor, with values rounded and clamped to fit within</span>
<span class="sd">                      the 4-bit integer range.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">is_scale_none</span> <span class="o">=</span> <span class="n">scale_a</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale_a</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scale_a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="mf">5.6345</span>  <span class="c1"># Adjusted scale calculation for 4-bit</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.00001</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">scale_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scale_a</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">,</span> <span class="n">scale_a</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">Qn</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>
    <span class="n">Qp</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="k">if</span> <span class="n">is_scale_none</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">scale_a</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">Qn</span><span class="p">,</span> <span class="n">Qp</span><span class="p">),</span> <span class="n">scale_a</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">scale_a</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">Qn</span><span class="p">,</span> <span class="n">Qp</span><span class="p">)</span></div>



<div class="viewcode-block" id="gptq_stype_unpacking">
<a class="viewcode-back" href="../../../_autosummary/bitorch_engine.utils.quant_operators.gptq_stype_unpacking.html#bitorch_engine.utils.quant_operators.gptq_stype_unpacking">[docs]</a>
<span class="k">def</span> <span class="nf">gptq_stype_unpacking</span><span class="p">(</span><span class="n">qweight</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstructs the fp16 weight tensor from the input quantized weight parameter in GPTQ style.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        qweight: The quantized weight parameter object containing all necessary quantization information.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The reconstructed weight tensor in fp16 format.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">wf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">qweight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">qweight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span> <span class="o">//</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                   <span class="n">wf</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span> <span class="k">if</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span> <span class="o">==</span> <span class="mi">8</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">qweight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">qweight</span><span class="o">.</span><span class="n">asym</span><span class="p">:</span>
        <span class="n">zeros_unpack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">qweight</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span> <span class="o">//</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span><span class="p">),</span>
                             <span class="n">wf</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span> <span class="k">if</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span> <span class="o">==</span> <span class="mi">8</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">zeros_unpack</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">qweight</span><span class="o">.</span><span class="n">w_bit</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">zeros_unpack</span><span class="p">)</span>
        <span class="n">zeros_unpack</span> <span class="o">=</span> <span class="n">zeros_unpack</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">zeros</span> <span class="o">=</span> <span class="n">zeros_unpack</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">qweight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="n">qweight</span><span class="o">.</span><span class="n">scales</span><span class="p">[</span><span class="n">qweight</span><span class="o">.</span><span class="n">g_idx</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span> <span class="o">*</span> <span class="p">(</span><span class="n">weight</span> <span class="o">-</span> <span class="n">zeros</span><span class="p">[</span><span class="n">qweight</span><span class="o">.</span><span class="n">g_idx</span><span class="o">.</span><span class="n">long</span><span class="p">()])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 2. GPTQ style without g_index.</span>
        <span class="k">if</span> <span class="n">qweight</span><span class="o">.</span><span class="n">g_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scales</span> <span class="o">=</span> <span class="n">qweight</span><span class="o">.</span><span class="n">scales</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">//</span><span class="n">qweight</span><span class="o">.</span><span class="n">scales</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">qweight</span><span class="o">.</span><span class="n">scales</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">zeros</span> <span class="o">=</span> <span class="n">qweight</span><span class="o">.</span><span class="n">zeros</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">//</span> <span class="n">qweight</span><span class="o">.</span><span class="n">zeros</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">qweight</span><span class="o">.</span><span class="n">zeros</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span> <span class="o">-</span> <span class="n">zeros</span>
            <span class="n">q_perm</span> <span class="o">=</span> <span class="n">qweight</span><span class="o">.</span><span class="n">q_perm</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">q_perm</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">weights</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">qweight</span><span class="o">.</span><span class="n">scales</span><span class="p">[</span><span class="n">qweight</span><span class="o">.</span><span class="n">g_idx</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span> <span class="o">-</span> <span class="n">qweight</span><span class="o">.</span><span class="n">zeros</span><span class="p">[</span><span class="n">qweight</span><span class="o">.</span><span class="n">g_idx</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>

    <span class="k">return</span> <span class="n">weights</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Haojin Yang, Joseph Bethge, Nianhui Guo, Maximilian Schulze, Hong Guo, Paul Mattes.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>